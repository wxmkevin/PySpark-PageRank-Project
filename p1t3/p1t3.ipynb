{"cells":[{"cell_type":"code","execution_count":null,"id":"7e136982","metadata":{"id":"7e136982"},"outputs":[],"source":["from pyspark.sql import SparkSession\n","spark = SparkSession.builder.getOrCreate()\n","\n","import os\n","os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.databricks:spark-xml_2.12:0.14.0 pyspark-shell'\n","\n","temp = spark.read.csv('gs://4121programminghw2/temp_small_file.csv')\n","temp = temp.selectExpr(\"_c0 as title\", \"_c1 as articles\")\n","temp = temp.dropna()\n","\n","\n","from pyspark.sql.functions import lit, expr, sum, asc\n","df1 = temp.groupby('title').count()\n","df1 = df1.withColumn('rank', lit(1))\n","df1 = df1.selectExpr(\"title as title\", \"count as neighbors\", \"rank as rank\")\n","for _ in range(10):\n","    df2 = temp.join(df1, on='title', how='right')\n","    df2 = df2.withColumn('contri', expr('rank/neighbors'))\n","    df3 = df2.select('articles', 'contri').groupby('articles').agg(sum('contri').alias('contri'))\n","    df3 = df3.withColumn('rank', expr('0.85*contri+0.15')).dropna()\n","    df1 = df1.select('title', 'neighbors').join(df3.selectExpr('articles as title', 'rank as rank'), \n","                                            on='title', how='right').na.fill({'neighbors': 0})\n","\n","\n","df1.select(\"title\", \"rank\").sort(asc(\"title\"), asc(\"rank\")).limit(5).write.csv('gs://4121programminghw2/pr_small.csv', sep='\\t')"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.13"},"colab":{"name":"p1t3.ipynb","provenance":[],"collapsed_sections":[]}},"nbformat":4,"nbformat_minor":5}